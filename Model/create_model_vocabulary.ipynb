{"cells":[{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import pandas as pd\n","import cv2\n","import os\n","from glob import glob"],"outputs":[],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":["# **image Preprocess**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["images_path = '../input/flickr8k-sau/Flickr_Data/Images/'\n","images = glob(images_path+'*.jpg')\n","len(images)"],"outputs":[],"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true}},{"cell_type":"code","execution_count":null,"source":["images[:5]"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","\n","for i in range(5):\n","    plt.figure()\n","    img = cv2.imread(images[i])\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.imshow(img)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["from keras.applications import ResNet50\n","\n","incept_model = ResNet50(include_top=True)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["from keras.models import Model\n","last = incept_model.layers[-2].output\n","modele = Model(inputs = incept_model.input,outputs = last)\n","modele.summary()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["images_features = {}\n","count = 0\n","for i in images:\n","    img = cv2.imread(i)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(img, (224,224))\n","    \n","    img = img.reshape(1,224,224,3)\n","    pred = modele.predict(img).reshape(2048,)\n","        \n","    img_name = i.split('/')[-1]\n","    \n","    images_features[img_name] = pred\n","    \n","    count += 1\n","    \n","    if count > 1499:\n","        break\n","        \n","    elif count % 50 == 0:\n","        print(count)\n","    \n","        \n","    "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["len(images_features)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# **Text Preprocess**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["caption_path = '../input/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["captions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["len(captions)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["captions_dict = {}\n","for i in captions:\n","    try:\n","        img_name = i.split('\\t')[0][:-2] \n","        caption = i.split('\\t')[1]\n","        if img_name in images_features:\n","            if img_name not in captions_dict:\n","                captions_dict[img_name] = [caption]\n","                \n","            else:\n","                captions_dict[img_name].append(caption)\n","            \n","    except:\n","        pass"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["len(captions_dict)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# **Visualize Images with captions**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","\n","for i in range(5):\n","    plt.figure()\n","    img_name = images[i]\n","    \n","    \n","    img = cv2.imread(img_name)\n","    \n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.xlabel(captions_dict[img_name.split('/')[-1]])\n","    plt.imshow(img)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","\n","for k in images_features.keys():\n","    plt.figure()\n","    \n","    img_name = '../input/flickr8k-sau/Flickr_Data/Images/' + k\n","    \n","    \n","    img = cv2.imread(img_name)\n","    \n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.xlabel(captions_dict[img_name.split('/')[-1]])\n","    plt.imshow(img)\n","    \n","    break"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["\n","def preprocessed(txt):\n","    modified = txt.lower()\n","    modified = 'startofseq ' + modified + ' endofseq'\n","    return modified\n","    "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["for k,v in captions_dict.items():\n","    for vv in v:\n","        captions_dict[k][v.index(vv)] = preprocessed(vv)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# **Create Vocabulary**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["count_words = {}\n","for k,vv in captions_dict.items():\n","    for v in vv:\n","        for word in v.split():\n","            if word not in count_words:\n","\n","                count_words[word] = 0\n","\n","            else:\n","                count_words[word] += 1"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["len(count_words)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["THRESH = -1\n","count = 1\n","new_dict = {}\n","for k,v in count_words.items():\n","    if count_words[k] > THRESH:\n","        new_dict[k] = count\n","        count += 1\n","        "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["len(new_dict)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["new_dict['<OUT>'] = len(new_dict) "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["captions_backup = captions_dict.copy()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["captions_dict = captions_backup.copy()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["for k, vv in captions_dict.items():\n","    for v in vv:\n","        encoded = []\n","        for word in v.split():  \n","            if word not in new_dict:\n","                encoded.append(new_dict['<OUT>'])\n","            else:\n","                encoded.append(new_dict[word])\n","\n","\n","        captions_dict[k][vv.index(v)] = encoded"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["captions_dict"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["------------------------------------------------------------------------------------------------------"],"metadata":{}},{"cell_type":"markdown","source":["# **Build Generator Function**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from keras.utils import to_categorical\n","from keras.preprocessing.sequence import pad_sequences"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["MAX_LEN = 0\n","for k, vv in captions_dict.items():\n","    for v in vv:\n","        if len(v) > MAX_LEN:\n","            MAX_LEN = len(v)\n","            print(v)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["MAX_LEN"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["captions_dict"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["Batch_size = 5000\n","VOCAB_SIZE = len(new_dict)\n","\n","def generator(photo, caption):\n","    n_samples = 0\n","    \n","    X = []\n","    y_in = []\n","    y_out = []\n","    \n","    for k, vv in caption.items():\n","        for v in vv:\n","            for i in range(1, len(v)):\n","                X.append(photo[k])\n","\n","                in_seq= [v[:i]]\n","                out_seq = v[i]\n","\n","                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]\n","                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n","\n","                y_in.append(in_seq)\n","                y_out.append(out_seq)\n","            \n","    return X, y_in, y_out\n","    \n","    "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["X, y_in, y_out = generator(images_features, captions_dict)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["len(X), len(y_in), len(y_out)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["X = np.array(X)\n","y_in = np.array(y_in, dtype='float64')\n","y_out = np.array(y_out, dtype='float64')\n","\n"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["X.shape, y_in.shape, y_out.shape"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["X[1510]"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["y_in[2]"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# **MODEL**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils import plot_model\n","from keras.models import Model, Sequential\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","from keras.layers.merge import add\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n","from keras.models import Sequential, Model"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["embedding_size = 128\n","max_len = MAX_LEN\n","vocab_size = len(new_dict)\n","\n","image_model = Sequential()\n","\n","image_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\n","image_model.add(RepeatVector(max_len))\n","\n","image_model.summary()\n","\n","language_model = Sequential()\n","\n","language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\n","language_model.add(LSTM(256, return_sequences=True))\n","language_model.add(TimeDistributed(Dense(embedding_size)))\n","\n","language_model.summary()\n","\n","conca = Concatenate()([image_model.output, language_model.output])\n","x = LSTM(128, return_sequences=True)(conca)\n","x = LSTM(512, return_sequences=False)(x)\n","x = Dense(vocab_size)(x)\n","out = Activation('softmax')(x)\n","model = Model(inputs=[image_model.input, language_model.input], outputs = out)\n","\n","# model.load_weights(\"../input/model_weights.h5\")\n","model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n","model.summary()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["model.fit([X, y_in], y_out, batch_size=512, epochs=50)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["inv_dict = {v:k for k, v in new_dict.items()}"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["model.save('model.h5')"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["model.save_weights('mine_model_weights.h5')"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["np.save('vocab.npy', new_dict)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["def getImage(x):\n","    \n","    test_img_path = images[x]\n","\n","    test_img = cv2.imread(test_img_path)\n","    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n","\n","    test_img = cv2.resize(test_img, (299,299))\n","\n","    test_img = np.reshape(test_img, (1,299,299,3))\n","    \n","    return test_img"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# **Predictions**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["for i in range(5):\n","    \n","    no = np.random.randint(1500,7000,(1,1))[0,0]\n","    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n","    \n","    test_img_path = images[no]\n","    test_img = cv2.imread(test_img_path)\n","    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n","\n","\n","    text_inp = ['startofseq']\n","\n","    count = 0\n","    caption = ''\n","    while count < 25:\n","        count += 1\n","\n","        encoded = []\n","        for i in text_inp:\n","            encoded.append(new_dict[i])\n","\n","        encoded = [encoded]\n","\n","        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n","\n","\n","        prediction = np.argmax(model.predict([test_feature, encoded]))\n","\n","        sampled_word = inv_dict[prediction]\n","\n","        caption = caption + ' ' + sampled_word\n","            \n","        if sampled_word == 'endofseq':\n","            break\n","\n","        text_inp.append(sampled_word)\n","        \n","    plt.figure()\n","    plt.imshow(test_img)\n","    plt.xlabel(caption)"],"outputs":[],"metadata":{"trusted":true}}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}